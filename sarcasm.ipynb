{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30408928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from transformers import BertTokenizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences \n",
    "  \n",
    "from sklearn.metrics import classification_report, confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e26432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv('Sarcasm data.txt', sep='\\t', header=None, usecols=[0, 1])\n",
    "df.columns = ['text', 'category']\n",
    "\n",
    "# Drop rows with missing values and empty text\n",
    "df = df.dropna()\n",
    "df = df[df['text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1446e40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5250, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94308bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "                                                text category\n",
      "0          Triple Talaq par Burbak Kuchh nahi bolega       NO\n",
      "1  Batao ye uss site pr se akki sir ke verdict ni...      YES\n",
      "2  Hindu baheno par julam bardas nahi hoga @Tripl...       NO\n",
      "3  Naa bhai.. aisa nhi hai.. mere handle karne se...       NO\n",
      "4  #RememberingRajiv aaj agar musalman auraten tr...       NO\n",
      "\n",
      "Validation Set:\n",
      "                                                 text category\n",
      "1   Batao ye uss site pr se akki sir ke verdict ni...      YES\n",
      "4   #RememberingRajiv aaj agar musalman auraten tr...       NO\n",
      "8   Bachcho ki death par politics ke bajay unke li...       NO\n",
      "11  #Bollywood @amitdey10510709   Bhaag Milkha Bhaag!       NO\n",
      "13  Bhai kuchh bhi karna iss @SimplySajidK ke saat...       NO\n",
      "\n",
      "Test Set:\n",
      "                                                text category\n",
      "0          Triple Talaq par Burbak Kuchh nahi bolega       NO\n",
      "2  Hindu baheno par julam bardas nahi hoga @Tripl...       NO\n",
      "3  Naa bhai.. aisa nhi hai.. mere handle karne se...       NO\n",
      "5  are cricket se sanyas le liya kya viru aur soc...       NO\n",
      "6  Sohail bahi hame bhi treen kardo muje bhi shoc...       NO\n"
     ]
    }
   ],
   "source": [
    "# Split the data using KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(df['text']):\n",
    "    break\n",
    "\n",
    "train_df = df.iloc[train_index]\n",
    "\n",
    "# Further split the test set into validation and test sets using KFold\n",
    "kf2 = KFold(n_splits=2, shuffle=True, random_state=42)\n",
    "for val_index, test_index in kf2.split(df.iloc[test_index]['text']):\n",
    "    break\n",
    "\n",
    "val_df = df.iloc[val_index]\n",
    "test_df = df.iloc[test_index]\n",
    "\n",
    "# Display the first few rows of each split\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_df.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6815d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n",
      "                                                text category\n",
      "0          triple talaq par burbak kuchh nahi bolega       NO\n",
      "1  batao ye uss site pr se akki sir ke verdict ni...      YES\n",
      "2  hindu baheno par julam bardas nahi hoga hindu ...       NO\n",
      "3  naa bhai aisa nhi hai mere handle karne se bhi...       NO\n",
      "4  aaj agar musalman auraten triple talaq ki waja...       NO\n",
      "\n",
      "Validation Set:\n",
      "                                                 text category\n",
      "1   batao ye uss site pr se akki sir ke verdict ni...      YES\n",
      "4   aaj agar musalman auraten triple talaq ki waja...       NO\n",
      "8   bachcho ki death par politics ke bajay unke li...       NO\n",
      "11                                 bhaag milkha bhaag       NO\n",
      "13  bhai kuchh bhi karna iss ke saath movie mat ka...       NO\n",
      "\n",
      "Test Set:\n",
      "                                                text category\n",
      "0          triple talaq par burbak kuchh nahi bolega       NO\n",
      "2  hindu baheno par julam bardas nahi hoga hindu ...       NO\n",
      "3  naa bhai aisa nhi hai mere handle karne se bhi...       NO\n",
      "5  are cricket se sanyas le liya kya viru aur soc...       NO\n",
      "6  sohail bahi hame bhi treen kardo muje bhi shoc...       NO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text'] = train_df['text'].apply(clean_tweets)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text'] = train_df['text'].apply(remove_html)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text'] = train_df['text'].apply(remove_email)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text'] = train_df['text'].apply(remove_all_special_chars)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:45: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text'] = train_df['text'].apply(replace_mult_spaces)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['text'] = train_df['text'].apply(lambda x: replace_chars(x, '[()!@&;]'))\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['text'] = val_df['text'].apply(clean_tweets)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['text'] = val_df['text'].apply(remove_html)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['text'] = val_df['text'].apply(remove_email)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['text'] = val_df['text'].apply(remove_all_special_chars)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['text'] = val_df['text'].apply(replace_mult_spaces)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:53: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['text'] = val_df['text'].apply(lambda x: replace_chars(x, '[()!@&;]'))\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['text'] = test_df['text'].apply(clean_tweets)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['text'] = test_df['text'].apply(remove_html)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['text'] = test_df['text'].apply(remove_email)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:58: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['text'] = test_df['text'].apply(remove_all_special_chars)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:59: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['text'] = test_df['text'].apply(replace_mult_spaces)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\310629052.py:60: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['text'] = test_df['text'].apply(lambda x: replace_chars(x, '[()!@&;]'))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the cleaning functions\n",
    "def clean_tweets(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def remove_html(text):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    pattern = re.compile('<.*?>')  # all the HTML tags\n",
    "    return pattern.sub(r'', text)\n",
    "\n",
    "def remove_email(text):\n",
    "    text = re.sub(r'[\\w.<>]\\w+@\\w+[\\w.<>]', \" \", text)\n",
    "    return text\n",
    "\n",
    "def remove_all_special_chars(text):\n",
    "    text = re.sub(\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def replace_mult_spaces(text):\n",
    "    text = text.replace(\"&quot\", \"\")\n",
    "    pattern = re.compile(' +')\n",
    "    text = pattern.sub(r' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def replace_chars(text, pattern):\n",
    "    pattern = re.compile(pattern)\n",
    "    text = pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "# Load or define the DataFrames train_df, val_df, and test_df here...\n",
    "\n",
    "# Apply cleaning functions to the 'Tweet' column of each DataFrame\n",
    "train_df['text'] = train_df['text'].apply(clean_tweets)\n",
    "train_df['text'] = train_df['text'].apply(remove_html)\n",
    "train_df['text'] = train_df['text'].apply(remove_email)\n",
    "train_df['text'] = train_df['text'].apply(remove_all_special_chars)\n",
    "train_df['text'] = train_df['text'].apply(replace_mult_spaces)\n",
    "train_df['text'] = train_df['text'].apply(lambda x: replace_chars(x, '[()!@&;]'))\n",
    "\n",
    "val_df['text'] = val_df['text'].apply(clean_tweets)\n",
    "val_df['text'] = val_df['text'].apply(remove_html)\n",
    "val_df['text'] = val_df['text'].apply(remove_email)\n",
    "val_df['text'] = val_df['text'].apply(remove_all_special_chars)\n",
    "val_df['text'] = val_df['text'].apply(replace_mult_spaces)\n",
    "val_df['text'] = val_df['text'].apply(lambda x: replace_chars(x, '[()!@&;]'))\n",
    "\n",
    "test_df['text'] = test_df['text'].apply(clean_tweets)\n",
    "test_df['text'] = test_df['text'].apply(remove_html)\n",
    "test_df['text'] = test_df['text'].apply(remove_email)\n",
    "test_df['text'] = test_df['text'].apply(remove_all_special_chars)\n",
    "test_df['text'] = test_df['text'].apply(replace_mult_spaces)\n",
    "test_df['text'] = test_df['text'].apply(lambda x: replace_chars(x, '[()!@&;]'))\n",
    "\n",
    "# Display the first few rows of each DataFrame to verify the changes\n",
    "print(\"Train Set:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nValidation Set:\")\n",
    "print(val_df.head())\n",
    "print(\"\\nTest Set:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27305de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\1009818470.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['category'], label2idx = convert_categorical_label_to_int(train_df['category'].values)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\1009818470.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  val_df['category'], _ = convert_categorical_label_to_int(val_df['category'].values)\n",
      "C:\\Users\\nikit\\AppData\\Local\\Temp\\ipykernel_17492\\1009818470.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['category'], _ = convert_categorical_label_to_int(test_df['category'].values)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>triple talaq par burbak kuchh nahi bolega</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>batao ye uss site pr se akki sir ke verdict ni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hindu baheno par julam bardas nahi hoga hindu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>naa bhai aisa nhi hai mere handle karne se bhi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aaj agar musalman auraten triple talaq ki waja...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  category\n",
       "0          triple talaq par burbak kuchh nahi bolega         0\n",
       "1  batao ye uss site pr se akki sir ke verdict ni...         1\n",
       "2  hindu baheno par julam bardas nahi hoga hindu ...         0\n",
       "3  naa bhai aisa nhi hai mere handle karne se bhi...         0\n",
       "4  aaj agar musalman auraten triple talaq ki waja...         0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def _get_unique(elems):\n",
    "    if type(elems[0]) == list:\n",
    "        corpus = [item for sublist in elems for item in sublist]\n",
    "    else:\n",
    "        corpus = elems\n",
    "    elems, freqs = zip(*Counter(corpus).most_common())\n",
    "    return list(elems)\n",
    "\n",
    "def convert_categorical_label_to_int(labels):\n",
    "    if type(labels[0]) == list:\n",
    "        uniq_labels = _get_unique(labels)\n",
    "    else:\n",
    "        uniq_labels = _get_unique(labels)\n",
    "\n",
    "    label_to_id = {}\n",
    "    if type(labels[0]) == list:\n",
    "        label_to_id = {w: i+1 for i, w in enumerate(uniq_labels)}\n",
    "    else:\n",
    "        label_to_id = {w: i for i, w in enumerate(uniq_labels)}\n",
    "\n",
    "    new_labels = []\n",
    "    if type(labels[0]) == list:\n",
    "        for i in labels:\n",
    "            new_labels.append([label_to_id[j] for j in i])\n",
    "    else:\n",
    "        new_labels = [label_to_id[j] for j in labels]\n",
    "\n",
    "    return new_labels, label_to_id\n",
    "\n",
    "# Convert categorical labels to integer values\n",
    "train_df['category'], label2idx = convert_categorical_label_to_int(train_df['category'].values)\n",
    "\n",
    "val_df['category'], _ = convert_categorical_label_to_int(val_df['category'].values)\n",
    "\n",
    "test_df['category'], _ = convert_categorical_label_to_int(test_df['category'].values)\n",
    "\n",
    "# Display the first few rows of each DataFrame to verify the changes\n",
    "print(\"Train Set:\")\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f172c394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TF-IDF Shape:   (0, 4628)\t0.24722599243672708\n",
      "  (0, 4483)\t0.24484836196955587\n",
      "  (0, 3463)\t0.33138722732960807\n",
      "  (0, 3071)\t0.2515423489851793\n",
      "  (0, 2565)\t0.5021339612553597\n",
      "  (0, 806)\t0.6735718413712931\n",
      "  (1, 4932)\t0.1216209708168168\n",
      "  (1, 4777)\t0.2783739246220301\n",
      "  (1, 4735)\t0.2518095301176143\n",
      "  (1, 4327)\t0.30493831912644587\n",
      "  (1, 4321)\t0.16057071090266997\n",
      "  (1, 4179)\t0.09875741022155622\n",
      "  (1, 3702)\t0.20115413605910926\n",
      "  (1, 3256)\t0.3159635389914152\n",
      "  (1, 3075)\t0.18314156646933635\n",
      "  (1, 2990)\t0.22524513561319853\n",
      "  (1, 2484)\t0.08637668337476942\n",
      "  (1, 2357)\t0.0963613600406013\n",
      "  (1, 2002)\t0.2661694018567466\n",
      "  (1, 1760)\t0.27385996984755306\n",
      "  (1, 1720)\t0.16057071090266997\n",
      "  (1, 1366)\t0.14253046032923378\n",
      "  (1, 757)\t0.2379014237694122\n",
      "  (1, 583)\t0.24522348584200004\n",
      "  (1, 193)\t0.2698220997073872\n",
      "  :\t:\n",
      "  (4195, 2580)\t0.22806915470252515\n",
      "  (4196, 4604)\t0.15680410915740645\n",
      "  (4196, 3991)\t0.3695741542148875\n",
      "  (4196, 3863)\t0.49084177738539736\n",
      "  (4196, 3176)\t0.5149815123797418\n",
      "  (4196, 3075)\t0.28450602967007965\n",
      "  (4196, 1635)\t0.10405035330277594\n",
      "  (4196, 1253)\t0.49084177738539736\n",
      "  (4197, 4551)\t0.724158327962075\n",
      "  (4197, 1986)\t0.4190784750206793\n",
      "  (4197, 1968)\t0.5226728014055497\n",
      "  (4197, 1635)\t0.16364929113316856\n",
      "  (4198, 4548)\t0.30563123821587074\n",
      "  (4198, 3064)\t0.6095827542027397\n",
      "  (4198, 2936)\t0.4528138195642412\n",
      "  (4198, 2727)\t0.2587499723360939\n",
      "  (4198, 2529)\t0.26689821833362987\n",
      "  (4198, 339)\t0.19546194427938302\n",
      "  (4198, 108)\t0.3918755899795327\n",
      "  (4199, 4932)\t0.24978889990606334\n",
      "  (4199, 4916)\t0.5541689480325462\n",
      "  (4199, 4548)\t0.32536190950195504\n",
      "  (4199, 4029)\t0.6262917222435685\n",
      "  (4199, 2918)\t0.302396043421678\n",
      "  (4199, 730)\t0.20237890684940707\n",
      "Validation TF-IDF Shape: (525, 5000)\n",
      "Test TF-IDF Shape: (525, 5000)\n",
      "Train Sequences Shape: [[  16   12   28 ...    0    0    0]\n",
      " [ 488   18  630 ...    0    0    0]\n",
      " [ 127 2306   28 ...    0    0    0]\n",
      " ...\n",
      " [  73 1902    1 ...    0    0    0]\n",
      " [ 163    9   27 ...    0    0    0]\n",
      " [2484   37 1012 ...    0    0    0]]\n",
      "Validation Sequences Shape: (525, 32)\n",
      "Test Sequences Shape: (525, 32)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "tfidf_vectorizer.fit(train_df['text'])\n",
    "\n",
    "train_tfidf = tfidf_vectorizer.transform(train_df['text'])\n",
    "val_tfidf = tfidf_vectorizer.transform(val_df['text'])\n",
    "test_tfidf = tfidf_vectorizer.transform(test_df['text'])\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_df['text'])\n",
    "\n",
    "# Convert text data to sequences\n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "val_sequences = tokenizer.texts_to_sequences(val_df['text'])\n",
    "test_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "\n",
    "# Padding sequences to ensure uniform length\n",
    "max_len = max([len(seq) for seq in train_sequences])\n",
    "train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')\n",
    "val_sequences = pad_sequences(val_sequences, maxlen=max_len, padding='post')\n",
    "test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')\n",
    "\n",
    "# Display shape of the data to verify\n",
    "print(\"Train TF-IDF Shape:\", train_tfidf)\n",
    "print(\"Validation TF-IDF Shape:\", val_tfidf.shape)\n",
    "print(\"Test TF-IDF Shape:\", test_tfidf.shape)\n",
    "\n",
    "print(\"Train Sequences Shape:\", train_sequences)\n",
    "print(\"Validation Sequences Shape:\", val_sequences.shape)\n",
    "print(\"Test Sequences Shape:\", test_sequences.shape)\n",
    "\n",
    "# Now, you can proceed with training your LSTM model using these preprocessed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0246872e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikit\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d                 │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ ?                           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_max_pooling1d                 │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling1D\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ ?                           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "vocab_size = 10000\n",
    "  \n",
    "# Embedding dimension value \n",
    "embedding_dim = 200\n",
    "  \n",
    "# Max length of sentence \n",
    "max_length = 60\n",
    "  \n",
    "# pad_sequences arg \n",
    "padding_type = 'post'# Import the TensorFlow library \n",
    "import tensorflow as tf \n",
    "  \n",
    "# Define a sequential neural network model \n",
    "model = tf.keras.Sequential([ \n",
    "    # Embedding layer for creating word embeddings \n",
    "    tf.keras.layers.Embedding( \n",
    "        vocab_size, embedding_dim, input_length=max_length), \n",
    "  \n",
    "    # GlobalMaxPooling layer to extract relevant features \n",
    "    tf.keras.layers.GlobalMaxPool1D(), \n",
    "  \n",
    "    # First Dense layer with 40 neurons and ReLU activation \n",
    "    tf.keras.layers.Dense(40, activation='relu'), \n",
    "  \n",
    "    # Dropout layer to prevent overfitting \n",
    "    tf.keras.layers.Dropout(0.5), \n",
    "  \n",
    "    # Second Dense layer with 20 neurons and ReLU activation \n",
    "    tf.keras.layers.Dense(20, activation='relu'), \n",
    "  \n",
    "    # Dropout layer to prevent overfitting \n",
    "    tf.keras.layers.Dropout(0.5), \n",
    "  \n",
    "    # Third Dense layer with 10 neurons and ReLU activation \n",
    "    tf.keras.layers.Dense(10, activation='relu'), \n",
    "  \n",
    "    # Dropout layer to prevent overfitting \n",
    "    tf.keras.layers.Dropout(0.2), \n",
    "  \n",
    "    # Final Dense layer with 1 neuron and sigmoid activation for binary classification \n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "]) \n",
    "  \n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4feed3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 32ms/step - accuracy: 0.8064 - loss: 0.5212 - val_accuracy: 0.9048 - val_loss: 0.2776\n",
      "Epoch 2/7\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 36ms/step - accuracy: 0.9017 - loss: 0.3065 - val_accuracy: 0.9048 - val_loss: 0.1519\n",
      "Epoch 3/7\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 38ms/step - accuracy: 0.9040 - loss: 0.1811 - val_accuracy: 0.9048 - val_loss: 0.1108\n",
      "Epoch 4/7\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 41ms/step - accuracy: 0.9180 - loss: 0.1261 - val_accuracy: 0.9810 - val_loss: 0.0762\n",
      "Epoch 5/7\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.9752 - loss: 0.0761 - val_accuracy: 0.9810 - val_loss: 0.0559\n",
      "Epoch 6/7\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9883 - loss: 0.0455 - val_accuracy: 0.9771 - val_loss: 0.0524\n",
      "Epoch 7/7\n",
      "\u001b[1m132/132\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 31ms/step - accuracy: 0.9953 - loss: 0.0282 - val_accuracy: 0.9905 - val_loss: 0.0342\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 944us/step - accuracy: 0.9747 - loss: 0.1042   \n",
      "Test Loss: 0.09353841841220856\n",
      "Test Accuracy: 0.9752380847930908\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_sequences, train_df['category'], \n",
    "                    epochs=7, batch_size=32, \n",
    "                    validation_data=(val_sequences, val_df['category']))\n",
    "\n",
    "# Evaluate the model on test set\n",
    "loss, accuracy = model.evaluate(test_sequences, test_df['category'])\n",
    "print(\"Test Loss:\", loss)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9015248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Prediction: Not Sarcasm\n"
     ]
    }
   ],
   "source": [
    "def predict_sarcasm(text):\n",
    "    # Clean and preprocess the input text\n",
    "    cleaned_text = clean_tweets(text)\n",
    "    cleaned_text = remove_html(cleaned_text)\n",
    "    cleaned_text = remove_email(cleaned_text)\n",
    "    cleaned_text = remove_all_special_chars(cleaned_text)\n",
    "    cleaned_text = replace_mult_spaces(cleaned_text)\n",
    "    cleaned_text = replace_chars(cleaned_text, '[()!@&;]')\n",
    "    \n",
    "    # Tokenize and pad the input sequence\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned_text])\n",
    "    sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(sequence)\n",
    "    \n",
    "    # Convert prediction to binary label\n",
    "    binary_prediction = 1 if prediction > 0.5 else 0\n",
    "    \n",
    "    # Return prediction\n",
    "    if binary_prediction == 1:\n",
    "        return \"Sarcasm\"\n",
    "    else:\n",
    "        return \"Not Sarcasm\"\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Hindu Married men to Mr. Modi: triple talaq toh nipat gaya ab woh saat janmoon wala masla bhi nipata dijeyega  @narendramodi\"\n",
    "prediction = predict_sarcasm(input_text)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc31e61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6013559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
